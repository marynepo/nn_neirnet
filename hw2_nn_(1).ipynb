{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2_nn (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GW1AdLKQQzs"
      },
      "source": [
        "# Домашнее задание"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGgow_ccFOtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611b0066-f759-48b6-c0dc-df3c3cdbb7bf"
      },
      "source": [
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-26 18:32:43--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2021-11-26 18:32:43--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-26 18:32:43 ERROR 404: Not Found.\n",
            "\n",
            "--2021-11-26 18:32:43--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2021-11-26 18:32:43--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-26 18:32:43 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEVljqCqFyDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceef2774-f12b-476a-caab-8cf7e3940ec9"
      },
      "source": [
        "!pip install torchmetrics\n",
        "!pip install ipdb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.7/dist-packages (0.13.9)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Requirement already satisfied: ipython>=7.17.0 in /usr/local/lib/python3.7/dist-packages (from ipdb) (7.29.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (3.0.22)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M1JZhNKD1j1",
        "outputId": "b5cbaeb6-5ea5-44a9-a88c-bd3efacb4ff5"
      },
      "source": [
        "!pip install pymystem3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pymystem3) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGpeCvnRFp-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953a2f83-13ef-491d-d6ab-ac163904f8fd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from string import punctuation\n",
        "import re\n",
        "import gensim\n",
        "import matplotlib as plt\n",
        "from pymystem3 import Mystem\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dJbtCPCBRF4",
        "outputId": "8827e500-9aa0-40e7-8b1b-589f6e2b0bec"
      },
      "source": [
        "torch.cuda.manual_seed_all(42)\n",
        "torch.manual_seed(42)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe585182250>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNfDbOxvQYFP"
      },
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb4f_VLzGGQ1"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0\n",
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "tweets_data = shuffle(all_tweets_data[['text','tone']])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTPI4CnFHYk2"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
        "    text = re.sub('@[^\\s]+', 'USER', text)\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "tweets_data['prep_text'] = [preprocess_text(text) for text in tweets_data['text']]\n",
        "train_data, val_data = train_test_split(tweets_data, test_size=0.2) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRGRCUJWI2Lh",
        "outputId": "790473fe-a651-490e-c7f9-e90fb1469028"
      },
      "source": [
        "word_vocab = Counter()\n",
        "for text in tweets_data['prep_text']:\n",
        "    word_vocab.update(word_tokenize(text))\n",
        "print('всего уникальных символов:', len(word_vocab))\n",
        "\n",
        "filtered_word_vocab = set()\n",
        "\n",
        "for word in word_vocab:\n",
        "    if word_vocab[word] > 2:\n",
        "        filtered_word_vocab.add(word)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_word_vocab))\n",
        "\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_word_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 180371\n",
            "уникальных символов, втретившихся больше 5 раз: 52157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn-78QnEHpY2",
        "outputId": "8e9386fc-6720-4032-c54a-807ef6a300c3"
      },
      "source": [
        "sym_vocab = Counter()\n",
        "for symbol in tweets_data['prep_text']:\n",
        "    sym_vocab.update(list(symbol))\n",
        "print('всего уникальных символов:', len(sym_vocab))\n",
        "\n",
        "filtered_sym_vocab = set()\n",
        "\n",
        "for symbol in sym_vocab:\n",
        "    if sym_vocab[symbol] > 5:\n",
        "        filtered_sym_vocab.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_sym_vocab))\n",
        "\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_sym_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 73\n",
            "уникальных символов, втретившихся больше 5 раз: 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19iZZuw8QgeT"
      },
      "source": [
        "## Создание датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z5rZiweHCnA"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll4tlX8cLOXc"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['prep_text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = word_tokenize(self.dataset[index])\n",
        "        ids = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, y"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRe_F-fOLyAg"
      },
      "source": [
        "train_dataset = TweetsDataset(train_data, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "val_dataset = TweetsDataset(val_data, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0jotQTzQpE6"
      },
      "source": [
        "## Создание модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy0U6djtMgb7"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3, padding='same')\n",
        "        self.grams = nn.Conv1d(in_channels=200, out_channels=80, kernel_size=2, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=80, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.pooling(self.relu(self.bigrams(embedded)))\n",
        "        feature_map_trigrams = self.pooling(self.relu(self.trigrams(embedded)))\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_grams = self.pooling(self.relu(self.grams(concat)))\n",
        "        pooling3 = feature_map_grams.max(2)[0]\n",
        "        logits = self.hidden(pooling3) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SzonQXjNDQh"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(texts)\n",
        "        loss = criterion(preds, ys)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in tqdm(enumerate(iterator)):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ' '.join([id2word[int(word)] for word in text if word !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    pr = len(tp) / (len(tp) + len(fp))\n",
        "    rc = len(tp) / (len(tp) + len(fn))\n",
        "    f1sc = 2*pr*rc / (pr + rc) \n",
        "    return fp, fn, tp, tn, {'precision': pr, 'recall': rc, 'f1-score': f1sc}"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YmhQXPaQu__"
      },
      "source": [
        "## Обучение и предсказание"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV1sJXaiNMJy"
      },
      "source": [
        "model = CNN(len(word2id), 10)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBIPig3bNxWu",
        "outputId": "c54e4306-625c-4f06-e93a-4b54b3347d4c"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.70350383309757\n",
            "Train loss: 0.685306992219842\n",
            "Train loss: 0.6756421356247022\n",
            "Train loss: 0.6683213749377848\n",
            "Train loss: 0.663205516064304\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.651031930656994, Val f1: 0.6876914501190186\n",
            "Val loss: 0.642157092474509, Val f1: 0.6772367358207703\n",
            "Val loss: 0.6395699582420863, Val f1: 0.6728603839874268\n",
            "Val loss: 0.6380118251704484, Val f1: 0.6713840961456299\n",
            "Val loss: 0.6369009000816565, Val f1: 0.6709521412849426\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.722320444881916, Val f1: 0.726077675819397\n",
            "Val loss: 0.6790135011953466, Val f1: 0.6896206736564636\n",
            "Val loss: 0.6636272577139047, Val f1: 0.6835275292396545\n",
            "Val loss: 0.6577770965439933, Val f1: 0.6771033406257629\n",
            "Val loss: 0.6530844582752748, Val f1: 0.6748011112213135\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6472050021676456\n",
            "Train loss: 0.633809052515721\n",
            "Train loss: 0.6282867588675939\n",
            "Train loss: 0.6244648399112893\n",
            "Train loss: 0.6204811444227722\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6185533351757947, Val f1: 0.7106002569198608\n",
            "Val loss: 0.6102985774261364, Val f1: 0.6973809003829956\n",
            "Val loss: 0.6082323732284399, Val f1: 0.6936109066009521\n",
            "Val loss: 0.6064376350787046, Val f1: 0.6926735043525696\n",
            "Val loss: 0.6056705494036619, Val f1: 0.6914719343185425\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6919738873839378, Val f1: 0.745853066444397\n",
            "Val loss: 0.6504143792040208, Val f1: 0.7082011699676514\n",
            "Val loss: 0.6360146976434268, Val f1: 0.7002927660942078\n",
            "Val loss: 0.6308277913502285, Val f1: 0.6934750080108643\n",
            "Val loss: 0.6265974315730009, Val f1: 0.6901447176933289\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6140497098950779\n",
            "Train loss: 0.6030336264250935\n",
            "Train loss: 0.5983737924924264\n",
            "Train loss: 0.5962385145022715\n",
            "Train loss: 0.5943095988925846\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5962286819429958, Val f1: 0.7059395909309387\n",
            "Val loss: 0.5871616187302963, Val f1: 0.6958975791931152\n",
            "Val loss: 0.5832236999502549, Val f1: 0.6930587291717529\n",
            "Val loss: 0.5812482409340014, Val f1: 0.6919438242912292\n",
            "Val loss: 0.580073009962323, Val f1: 0.6910272836685181\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6663812473416328, Val f1: 0.743943452835083\n",
            "Val loss: 0.6265027242548326, Val f1: 0.7055363655090332\n",
            "Val loss: 0.6130671799182892, Val f1: 0.6961386203765869\n",
            "Val loss: 0.6086914079529898, Val f1: 0.6900848150253296\n",
            "Val loss: 0.6047816588120027, Val f1: 0.6866698861122131\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.5959441959857941\n",
            "Train loss: 0.5865022591922594\n",
            "Train loss: 0.5796322530278792\n",
            "Train loss: 0.577307599053966\n",
            "Train loss: 0.575279468777536\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5778268663322225, Val f1: 0.6960974931716919\n",
            "Val loss: 0.5676812685054281, Val f1: 0.6892662644386292\n",
            "Val loss: 0.5651752845599101, Val f1: 0.6842895746231079\n",
            "Val loss: 0.5645726464635177, Val f1: 0.6816691756248474\n",
            "Val loss: 0.5630607570724926, Val f1: 0.681740939617157\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6501471251249313, Val f1: 0.7356476187705994\n",
            "Val loss: 0.6124678674866172, Val f1: 0.6940027475357056\n",
            "Val loss: 0.599686535505148, Val f1: 0.6840816736221313\n",
            "Val loss: 0.595964138848441, Val f1: 0.6764629483222961\n",
            "Val loss: 0.5925025018778715, Val f1: 0.6713838577270508\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5725754289066091\n",
            "Train loss: 0.5656195481618246\n",
            "Train loss: 0.5612376871017309\n",
            "Train loss: 0.5594104511274708\n",
            "Train loss: 0.5573541830325949\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5550401701646692, Val f1: 0.7383978366851807\n",
            "Val loss: 0.5492340974185778, Val f1: 0.7266539931297302\n",
            "Val loss: 0.546361631498887, Val f1: 0.7219398617744446\n",
            "Val loss: 0.5450560433401478, Val f1: 0.7197291254997253\n",
            "Val loss: 0.5441256347058834, Val f1: 0.7188993692398071\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6370197832584381, Val f1: 0.7703342437744141\n",
            "Val loss: 0.5998785460696501, Val f1: 0.7272286415100098\n",
            "Val loss: 0.5870231894346384, Val f1: 0.7168107032775879\n",
            "Val loss: 0.5831550172397069, Val f1: 0.7096230387687683\n",
            "Val loss: 0.5794801563024521, Val f1: 0.7057042717933655\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5566681623458862\n",
            "Train loss: 0.5480127714682317\n",
            "Train loss: 0.5448196278168604\n",
            "Train loss: 0.5430463496729624\n",
            "Train loss: 0.5410250867235249\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5398699176662108, Val f1: 0.7638253569602966\n",
            "Val loss: 0.5340415139993032, Val f1: 0.7510985136032104\n",
            "Val loss: 0.5307476348601855, Val f1: 0.747894823551178\n",
            "Val loss: 0.5295870557105798, Val f1: 0.7457669973373413\n",
            "Val loss: 0.5297181164396221, Val f1: 0.7440919280052185\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6304330080747604, Val f1: 0.7911756038665771\n",
            "Val loss: 0.5933484750635484, Val f1: 0.7485118508338928\n",
            "Val loss: 0.5798556437859168, Val f1: 0.7381589412689209\n",
            "Val loss: 0.5760099053382873, Val f1: 0.7311294674873352\n",
            "Val loss: 0.5721814375032078, Val f1: 0.7270846366882324\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5442808235392851\n",
            "Train loss: 0.53461126745611\n",
            "Train loss: 0.5307407980928054\n",
            "Train loss: 0.5283069960076174\n",
            "Train loss: 0.5264554554703592\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5245348583249485, Val f1: 0.7652096152305603\n",
            "Val loss: 0.5183748533760292, Val f1: 0.7535094022750854\n",
            "Val loss: 0.5144069080169384, Val f1: 0.7515307664871216\n",
            "Val loss: 0.5129514278696595, Val f1: 0.7492460608482361\n",
            "Val loss: 0.5129672432767933, Val f1: 0.7474219799041748\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6161515712738037, Val f1: 0.7917600870132446\n",
            "Val loss: 0.5810478329658508, Val f1: 0.7481496930122375\n",
            "Val loss: 0.5679854590159196, Val f1: 0.7380108833312988\n",
            "Val loss: 0.5643708654812404, Val f1: 0.7306076288223267\n",
            "Val loss: 0.5608161823316054, Val f1: 0.7261675000190735\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5244822879047955\n",
            "Train loss: 0.5165561169817827\n",
            "Train loss: 0.5130405036302713\n",
            "Train loss: 0.5116823866641779\n",
            "Train loss: 0.5108601673581135\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5118565804818097, Val f1: 0.7659449577331543\n",
            "Val loss: 0.5031172041443811, Val f1: 0.7557513117790222\n",
            "Val loss: 0.5005999551369593, Val f1: 0.751594066619873\n",
            "Val loss: 0.5002685279297314, Val f1: 0.7493153810501099\n",
            "Val loss: 0.4989853429383245, Val f1: 0.748217761516571\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6062729358673096, Val f1: 0.786532998085022\n",
            "Val loss: 0.5726507095729604, Val f1: 0.7437039613723755\n",
            "Val loss: 0.5598936401880704, Val f1: 0.7323464155197144\n",
            "Val loss: 0.556556715284075, Val f1: 0.7263377904891968\n",
            "Val loss: 0.5531703477556055, Val f1: 0.7219473719596863\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5141531649757834\n",
            "Train loss: 0.5050158872120623\n",
            "Train loss: 0.5026778805141265\n",
            "Train loss: 0.4994488491428842\n",
            "Train loss: 0.49693045242764483\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5033107540186714, Val f1: 0.7905679941177368\n",
            "Val loss: 0.492785157068916, Val f1: 0.7806626558303833\n",
            "Val loss: 0.4907498826774267, Val f1: 0.7777934074401855\n",
            "Val loss: 0.48900319432183137, Val f1: 0.775285542011261\n",
            "Val loss: 0.4878949397939375, Val f1: 0.7747060060501099\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6036576479673386, Val f1: 0.8134682178497314\n",
            "Val loss: 0.5705382052589866, Val f1: 0.7690751552581787\n",
            "Val loss: 0.5574181698835813, Val f1: 0.7580248117446899\n",
            "Val loss: 0.5534596528325763, Val f1: 0.7510148286819458\n",
            "Val loss: 0.5496637780557979, Val f1: 0.7467881441116333\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.4949273563483182\n",
            "Train loss: 0.4884295727031818\n",
            "Train loss: 0.4859901832846495\n",
            "Train loss: 0.48669349332507567\n",
            "Train loss: 0.48626131675709255\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4804493557004368, Val f1: 0.795183002948761\n",
            "Val loss: 0.47622472738874133, Val f1: 0.7817007303237915\n",
            "Val loss: 0.47394448346816576, Val f1: 0.7756143808364868\n",
            "Val loss: 0.4732214598346957, Val f1: 0.7732861042022705\n",
            "Val loss: 0.47292738416414154, Val f1: 0.7718075513839722\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5916189774870872, Val f1: 0.8065162301063538\n",
            "Val loss: 0.5595989087048698, Val f1: 0.7616912722587585\n",
            "Val loss: 0.5470031110139993, Val f1: 0.7493559122085571\n",
            "Val loss: 0.5436238697596959, Val f1: 0.7422394156455994\n",
            "Val loss: 0.5401751114563509, Val f1: 0.7383344769477844\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.4863336603431141\n",
            "Train loss: 0.4780158119789068\n",
            "Train loss: 0.4759423059339707\n",
            "Train loss: 0.4736419926873214\n",
            "Train loss: 0.4726758871612878\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4717267707866781, Val f1: 0.8064169883728027\n",
            "Val loss: 0.4670168524203093, Val f1: 0.7944763898849487\n",
            "Val loss: 0.4636905411115059, Val f1: 0.7903279662132263\n",
            "Val loss: 0.4622012533301072, Val f1: 0.7887495756149292\n",
            "Val loss: 0.4616997519443775, Val f1: 0.7874147891998291\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5885869413614273, Val f1: 0.8169353008270264\n",
            "Val loss: 0.5566274804227492, Val f1: 0.7718387246131897\n",
            "Val loss: 0.543862997339322, Val f1: 0.7626561522483826\n",
            "Val loss: 0.5403323113918305, Val f1: 0.7556157112121582\n",
            "Val loss: 0.5366191085089337, Val f1: 0.7515683770179749\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.47682179685901194\n",
            "Train loss: 0.46778220542963\n",
            "Train loss: 0.4649497903883457\n",
            "Train loss: 0.46411438339905775\n",
            "Train loss: 0.4621480900323254\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.45997992157936096, Val f1: 0.8154600858688354\n",
            "Val loss: 0.455129559489264, Val f1: 0.8019877076148987\n",
            "Val loss: 0.4525655350432946, Val f1: 0.7975503206253052\n",
            "Val loss: 0.45182484216827284, Val f1: 0.7946211695671082\n",
            "Val loss: 0.4512265483880865, Val f1: 0.7935746908187866\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5826338566839695, Val f1: 0.8211550712585449\n",
            "Val loss: 0.5520580410957336, Val f1: 0.7753846645355225\n",
            "Val loss: 0.5396611873920147, Val f1: 0.7646073698997498\n",
            "Val loss: 0.5358367119516645, Val f1: 0.7575582265853882\n",
            "Val loss: 0.5323183319785378, Val f1: 0.7531036138534546\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.4658340881852543\n",
            "Train loss: 0.4567653338114421\n",
            "Train loss: 0.4547319910847224\n",
            "Train loss: 0.4532141383174512\n",
            "Train loss: 0.45164402428714706\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4519457229796578, Val f1: 0.8143569827079773\n",
            "Val loss: 0.4452569108942281, Val f1: 0.8038814663887024\n",
            "Val loss: 0.4421412810110129, Val f1: 0.8000662922859192\n",
            "Val loss: 0.441125330521906, Val f1: 0.7978371977806091\n",
            "Val loss: 0.4400873088288581, Val f1: 0.7968697547912598\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5786239057779312, Val f1: 0.8232914209365845\n",
            "Val loss: 0.5488313173546511, Val f1: 0.775198757648468\n",
            "Val loss: 0.536247479227873, Val f1: 0.7639923095703125\n",
            "Val loss: 0.5327389410563877, Val f1: 0.7571746110916138\n",
            "Val loss: 0.5292948924682357, Val f1: 0.7526038885116577\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.45315394769696626\n",
            "Train loss: 0.4480548976124197\n",
            "Train loss: 0.4455506827395696\n",
            "Train loss: 0.4440560141484514\n",
            "Train loss: 0.44238286422586987\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.43902138313826394, Val f1: 0.8254696130752563\n",
            "Val loss: 0.43417916911235754, Val f1: 0.8121299147605896\n",
            "Val loss: 0.43297181794276607, Val f1: 0.8083141446113586\n",
            "Val loss: 0.431285850650115, Val f1: 0.8062326908111572\n",
            "Val loss: 0.430729679848956, Val f1: 0.8049871325492859\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.577293049544096, Val f1: 0.8280795216560364\n",
            "Val loss: 0.54799483628834, Val f1: 0.7806606888771057\n",
            "Val loss: 0.5350493616782702, Val f1: 0.7698063254356384\n",
            "Val loss: 0.5313578971794674, Val f1: 0.7629764080047607\n",
            "Val loss: 0.5275836912068453, Val f1: 0.759531557559967\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.44477419116917777\n",
            "Train loss: 0.4384973260803499\n",
            "Train loss: 0.4338075120288592\n",
            "Train loss: 0.43316339267243587\n",
            "Train loss: 0.4330648248908163\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.43280229235396667, Val f1: 0.8296011090278625\n",
            "Val loss: 0.42626976059830707, Val f1: 0.8172832131385803\n",
            "Val loss: 0.4229647090228704, Val f1: 0.8153501152992249\n",
            "Val loss: 0.42249475280157955, Val f1: 0.8133650422096252\n",
            "Val loss: 0.42143796007523593, Val f1: 0.811802327632904\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5739248991012573, Val f1: 0.8329033851623535\n",
            "Val loss: 0.5454766645151026, Val f1: 0.7827040553092957\n",
            "Val loss: 0.5325731612168826, Val f1: 0.7718419432640076\n",
            "Val loss: 0.5287237925188882, Val f1: 0.765285313129425\n",
            "Val loss: 0.5250038531693545, Val f1: 0.7616519927978516\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.43454868565587434\n",
            "Train loss: 0.4274050088032432\n",
            "Train loss: 0.4264838718450986\n",
            "Train loss: 0.4250620970194288\n",
            "Train loss: 0.42464555131978005\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4262865869437947, Val f1: 0.8277101516723633\n",
            "Val loss: 0.41868923237358313, Val f1: 0.8184126019477844\n",
            "Val loss: 0.4151585801289632, Val f1: 0.8144108057022095\n",
            "Val loss: 0.4144739617975496, Val f1: 0.8116622567176819\n",
            "Val loss: 0.4139369962544277, Val f1: 0.8105113506317139\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.571399912238121, Val f1: 0.8282884955406189\n",
            "Val loss: 0.5436762325903949, Val f1: 0.7776296734809875\n",
            "Val loss: 0.531338183925702, Val f1: 0.7669628262519836\n",
            "Val loss: 0.5279126967702593, Val f1: 0.7590091824531555\n",
            "Val loss: 0.5241825919259678, Val f1: 0.7552850246429443\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.4273262584910673\n",
            "Train loss: 0.4183109782744145\n",
            "Train loss: 0.41554464858311874\n",
            "Train loss: 0.415122360419884\n",
            "Train loss: 0.4143456588873918\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4212155140498105, Val f1: 0.8438869118690491\n",
            "Val loss: 0.4124025801817576, Val f1: 0.8336528539657593\n",
            "Val loss: 0.40984902387628186, Val f1: 0.8302280306816101\n",
            "Val loss: 0.40952863603187123, Val f1: 0.8273696303367615\n",
            "Val loss: 0.4089976915682869, Val f1: 0.8264685273170471\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5771849378943443, Val f1: 0.8418776988983154\n",
            "Val loss: 0.5489966816761914, Val f1: 0.7933751940727234\n",
            "Val loss: 0.5359845837721458, Val f1: 0.7832814455032349\n",
            "Val loss: 0.5321029671600886, Val f1: 0.7764665484428406\n",
            "Val loss: 0.5281078354878859, Val f1: 0.7724242210388184\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.42250677504960227\n",
            "Train loss: 0.41445689097694727\n",
            "Train loss: 0.4087086460338189\n",
            "Train loss: 0.40737891090001993\n",
            "Train loss: 0.40691485058987276\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4131938709932215, Val f1: 0.8467212319374084\n",
            "Val loss: 0.4047168493270874, Val f1: 0.8383085131645203\n",
            "Val loss: 0.4030557062763434, Val f1: 0.8338116407394409\n",
            "Val loss: 0.40260186598455305, Val f1: 0.8313760161399841\n",
            "Val loss: 0.4008929189936868, Val f1: 0.8305128812789917\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5776924267411232, Val f1: 0.8457938432693481\n",
            "Val loss: 0.5504592166227453, Val f1: 0.7971761226654053\n",
            "Val loss: 0.53689794586255, Val f1: 0.7859079837799072\n",
            "Val loss: 0.5325591317244939, Val f1: 0.7792339324951172\n",
            "Val loss: 0.5284407186237249, Val f1: 0.7757165431976318\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.40664089514928703\n",
            "Train loss: 0.4013446167759273\n",
            "Train loss: 0.4018705513041753\n",
            "Train loss: 0.40007015655366635\n",
            "Train loss: 0.3992171239578861\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3941345504101585, Val f1: 0.8512173295021057\n",
            "Val loss: 0.390853238278541, Val f1: 0.8359612226486206\n",
            "Val loss: 0.3890931715185826, Val f1: 0.8312426805496216\n",
            "Val loss: 0.38784958625868926, Val f1: 0.8291047811508179\n",
            "Val loss: 0.3879307285807599, Val f1: 0.8274751901626587\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5702896192669868, Val f1: 0.8343595862388611\n",
            "Val loss: 0.5442415475845337, Val f1: 0.784931480884552\n",
            "Val loss: 0.5312725913066131, Val f1: 0.7734271287918091\n",
            "Val loss: 0.5276996706213269, Val f1: 0.7661457657814026\n",
            "Val loss: 0.523776425556703, Val f1: 0.7624059319496155\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.4050099016988979\n",
            "Train loss: 0.3963416827761609\n",
            "Train loss: 0.39393934779442275\n",
            "Train loss: 0.3919414241108105\n",
            "Train loss: 0.39097020098532753\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3854161124019062, Val f1: 0.856972873210907\n",
            "Val loss: 0.38122098860533343, Val f1: 0.8440298438072205\n",
            "Val loss: 0.38219000313144463, Val f1: 0.8376631140708923\n",
            "Val loss: 0.3802165271138116, Val f1: 0.8363536596298218\n",
            "Val loss: 0.37986994731700285, Val f1: 0.8351600170135498\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5692580714821815, Val f1: 0.8394112586975098\n",
            "Val loss: 0.5432211125598234, Val f1: 0.7898189425468445\n",
            "Val loss: 0.5300020334812311, Val f1: 0.7787953615188599\n",
            "Val loss: 0.5263525102819715, Val f1: 0.7716946005821228\n",
            "Val loss: 0.5226170312274586, Val f1: 0.7677888870239258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KckoiVccOtxk",
        "outputId": "6d6eca1b-6a1f-47f5-9324-e1b554879d9e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "OMifwObVO2gJ",
        "outputId": "2ecfe08d-ea13-4271-a16d-a155cd77aae3"
      },
      "source": [
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX69xPGtbb72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d39047-2ed5-40e5-92e2-910b34fec26b"
      },
      "source": [
        "print(max(f1s_eval), np.array(f1s_eval).argmax())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7585, device='cuda:0') 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJzp3SCAMBrH",
        "outputId": "61f51d20-3e3f-4b68-88aa-6232f402bf68"
      },
      "source": [
        "fp, fn, tp, tn, metr = predict(model, val_iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "45it [01:25,  1.91s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNPO9saXQRnN",
        "outputId": "3b7c83f1-b888-432a-d1b9-eda2d483458f"
      },
      "source": [
        "metr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision': 0.7576965669988925,\n",
              " 'recall': 0.7447318007662835,\n",
              " 'f1-score': 0.7511582460531805}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os2uU6SFQWEc",
        "outputId": "a4de35cf-3940-4188-b625-25497df7d487"
      },
      "source": [
        "print(len(fp), len(fn), len(tp), len(tn))\n",
        "# большой разницы между tp и tn нет, чуть больше fn, чем fp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5470 5863 17105 16929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V65VBoHcQw0B",
        "outputId": "b7f6d78f-33b6-4ce6-d94a-067da9775232"
      },
      "source": [
        "fp[:20]\n",
        "#не всегда понимает иронию ('за минуту набежало четыре хороших человека которые тоже не видели этот великий сериал')\n",
        "#слово \"ужасный\" не удается  негативным (поможет ли лемматизация?)\n",
        "#вообще кажется, что он недооценивает негативность многих слов, с положительными все получше"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['как же беркуты домой поедут USER еще три полицейских автобуса в киеве URL URL',\n",
              " 'USER ну ладн не расстраивайся х что завтра делать планируешь',\n",
              " 'rt USER USER снов с',\n",
              " 'эх единственный номер который я помнил на придется забыть',\n",
              " 'USER в другой раз будь осторожнее',\n",
              " 'хочется стоять в толпе и кричать ура новому а вместо него по улице проходит дядя в под руки не болят от',\n",
              " 'за минуту набежало четыре хороших человека которые тоже не видели этот великий сериал',\n",
              " 'блять как же надоело жить от пятницы до пятницы от субботы до субботы заебало если честно URL',\n",
              " 'USER а с кем с мамой а дальше чтоо на боковую так неинтересно',\n",
              " 'USER ааа ну тогда молчу ты меня уделал',\n",
              " 'театр слишком ужасен у всех актеров из роли в роль одни и те же интонации и манеры игры',\n",
              " 'USER вернуться в феврале и стать еще лучше посмотрим я все еще не могу найти замену по',\n",
              " 'USER от такой коленки гематома на пол будет',\n",
              " 'бедные парни ваши 15 16 ти летние девочки сами не знают чего хотят от одного к другому набивают себе цену в какой раз столкнулся',\n",
              " 'USER USER USER USER или нужно подождать',\n",
              " 'обожемой с этим дитем можно фух первые шишки',\n",
              " 'USER да не получается я свой номер несколько раз использовал',\n",
              " 'как дела ребята URL',\n",
              " 'rt USER в треке новом который к стати в субботу столько души столько эмоций в него хули толку говорить сами все по',\n",
              " 'rt USER таможенники на украли у государства 14 млн URL мошенники чо']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuSAwh5-SN7G",
        "outputId": "518e9fb2-7623-40c5-be74-ef5a69939c31"
      },
      "source": [
        "fn[:20]\n",
        "#какие-то проблемы с изначальной разметкой, почти все предложения я бы тоже отнесла к негативным  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['посмотрел ну и хрень но джоли нравится',\n",
              " 'увидела маленьких подружек они за руку и болтали',\n",
              " 'задал учит 3 стиха и я рассказала 1 стих 2 раза и вот тебе две 5 ки',\n",
              " 'USER 1 тоже все время туда жму надо кнопку наклейку сделать',\n",
              " 'USER а от меня нет mtvstars 3 second to mars',\n",
              " 'USER зачем ты зубы',\n",
              " 'USER пак я до тебя доберусь в след году а пока',\n",
              " 'ну дальше по улице живет педофил которого вроде совсем не трогают ff follow',\n",
              " 'USER ахаххахаха а я как то не очень люблю',\n",
              " 'в горде пиздец как заметает я ебу нахуй жестко такая погода с братиком нам по душе',\n",
              " 'rt USER а вы еще спрашиваете почему я ничего не пишу',\n",
              " 'о этот огроменный ааааа 1 метр и 5 кг жесть больше половины меня',\n",
              " 'rt USER есть какая почта в погонять',\n",
              " 'rt USER вчера захватил настолько что я всю ночь его читал оторваться не мог теперь блин мне это вылазит боком и я ве',\n",
              " 'афиша город пишет про моих одногруппников пишет что они тупые ебланы грубо говоря',\n",
              " 'lt lt неужели окажется',\n",
              " 'USER я же тебе написала что пар не будет',\n",
              " 'для меня ощущение праздников приходит не с елками и украшениями а с снег я ненормальный',\n",
              " 'USER ложусь спать на работу рано вставать',\n",
              " 'ну вот тот самый компромисс я в первой ну что ж буду в субботу до 6 вечера сидеть в шараге URL']"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOMxEkc5sP_"
      },
      "source": [
        "Улучшение в самом конце"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXFqFEcw-glX"
      },
      "source": [
        "## Модель с предобученными эмбеддингами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "472xkO9b_HUx"
      },
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "ft = FastText(window=5, min_count=2, sentences=tweets_data['prep_text'])\n",
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = ft.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCW-Q94w-sMZ"
      },
      "source": [
        "class FCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3, padding='same')\n",
        "        self.grams = nn.Conv1d(in_channels=200, out_channels=80, kernel_size=2, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=80, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.pooling(self.relu(self.bigrams(embedded)))\n",
        "        feature_map_trigrams = self.pooling(self.relu(self.trigrams(embedded)))\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_grams = self.pooling(self.relu(self.grams(concat)))\n",
        "        pooling3 = feature_map_grams.max(2)[0]\n",
        "        logits = self.hidden(pooling3) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwHt3iXxyj8U",
        "outputId": "37f4e24b-6e50-40ac-ded1-0699afe19490"
      },
      "source": [
        "model = FCNN(len(word2id), 300)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)\n",
        "\n",
        "print(max(f1s_eval), np.array(f1s_eval).argmax())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.6430420139256645\n",
            "Train loss: 0.6023857368939165\n",
            "Train loss: 0.581617993231003\n",
            "Train loss: 0.5672408085075213\n",
            "Train loss: 0.5551946459487936\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.47664835610810447, Val f1: 0.7949100136756897\n",
            "Val loss: 0.4700301682603532, Val f1: 0.7846121191978455\n",
            "Val loss: 0.4686089908847442, Val f1: 0.78104168176651\n",
            "Val loss: 0.4680680295975088, Val f1: 0.7786738872528076\n",
            "Val loss: 0.4674306881838831, Val f1: 0.7778294086456299\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5624428875744343, Val f1: 0.8300518989562988\n",
            "Val loss: 0.531057910007589, Val f1: 0.7821490168571472\n",
            "Val loss: 0.5201458185911179, Val f1: 0.7709435820579529\n",
            "Val loss: 0.5154277861118317, Val f1: 0.7647826671600342\n",
            "Val loss: 0.5114186656745997, Val f1: 0.7606927156448364\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.48044052720069885\n",
            "Train loss: 0.4692219919052677\n",
            "Train loss: 0.46328807135040945\n",
            "Train loss: 0.45893549254472304\n",
            "Train loss: 0.45630135892451495\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40690090989365296, Val f1: 0.8531942963600159\n",
            "Val loss: 0.3943430643150772, Val f1: 0.8452206254005432\n",
            "Val loss: 0.39177710620256573, Val f1: 0.8409380912780762\n",
            "Val loss: 0.3907260673938038, Val f1: 0.8391076326370239\n",
            "Val loss: 0.38992828627427417, Val f1: 0.8377140164375305\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5282933227717876, Val f1: 0.8663472533226013\n",
            "Val loss: 0.5033080209704006, Val f1: 0.8108205199241638\n",
            "Val loss: 0.49305449770047116, Val f1: 0.7983492016792297\n",
            "Val loss: 0.48774189949035646, Val f1: 0.7926692962646484\n",
            "Val loss: 0.48445395651188766, Val f1: 0.7880217432975769\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.39164028360563163\n",
            "Train loss: 0.3873432317505712\n",
            "Train loss: 0.38611031295015263\n",
            "Train loss: 0.38702538931112496\n",
            "Train loss: 0.38745947284945126\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.32762820317464714, Val f1: 0.8965688943862915\n",
            "Val loss: 0.3216947118441264, Val f1: 0.8826742768287659\n",
            "Val loss: 0.31906027834002787, Val f1: 0.8786526322364807\n",
            "Val loss: 0.3183337947447523, Val f1: 0.8766613602638245\n",
            "Val loss: 0.31750198787656325, Val f1: 0.8756785988807678\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5263775512576103, Val f1: 0.8648895621299744\n",
            "Val loss: 0.5018334125771242, Val f1: 0.8139045834541321\n",
            "Val loss: 0.49008390651299405, Val f1: 0.8009389042854309\n",
            "Val loss: 0.48496248381478446, Val f1: 0.7954472303390503\n",
            "Val loss: 0.4819138679992069, Val f1: 0.7921965718269348\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.32679595929734845\n",
            "Train loss: 0.32239727567935333\n",
            "Train loss: 0.3243463956392728\n",
            "Train loss: 0.3241930451753328\n",
            "Train loss: 0.32552908412341414\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.25630201618461046, Val f1: 0.93046635389328\n",
            "Val loss: 0.2541741188885509, Val f1: 0.9178577065467834\n",
            "Val loss: 0.2530992955255967, Val f1: 0.9128273725509644\n",
            "Val loss: 0.25325991288363503, Val f1: 0.9100477695465088\n",
            "Val loss: 0.25232557524209737, Val f1: 0.9089787006378174\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5429013594985008, Val f1: 0.8595591187477112\n",
            "Val loss: 0.52100929442574, Val f1: 0.8073016405105591\n",
            "Val loss: 0.5076322624316583, Val f1: 0.796747088432312\n",
            "Val loss: 0.5024686387607029, Val f1: 0.7911182045936584\n",
            "Val loss: 0.4999954111196778, Val f1: 0.7871921062469482\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.25492702599833994\n",
            "Train loss: 0.2507144072349521\n",
            "Train loss: 0.25307200834728205\n",
            "Train loss: 0.25584709923044385\n",
            "Train loss: 0.25890342351691475\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.21418883344706366, Val f1: 0.9505271911621094\n",
            "Val loss: 0.21368742334669916, Val f1: 0.9349445104598999\n",
            "Val loss: 0.21328720794274256, Val f1: 0.9301761984825134\n",
            "Val loss: 0.21119568137813816, Val f1: 0.9288637638092041\n",
            "Val loss: 0.21067931734282394, Val f1: 0.9278056025505066\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6289355233311653, Val f1: 0.8758885860443115\n",
            "Val loss: 0.6036087064182057, Val f1: 0.8225798606872559\n",
            "Val loss: 0.5874060667478122, Val f1: 0.810833215713501\n",
            "Val loss: 0.5805986166000366, Val f1: 0.8049413561820984\n",
            "Val loss: 0.5766061679883436, Val f1: 0.8005592226982117\n",
            "tensor(0.7828, device='cuda:0') 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfFoD0oJ0NBv",
        "outputId": "97791d3e-c5e4-4529-ac5a-75e1005bc55d"
      },
      "source": [
        "fp, fn, tp, tn, metr = predict(model, val_iterator)\n",
        "metr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "45it [01:28,  1.97s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision': 0.7285928143712574,\n",
              " 'recall': 0.8476140717520028,\n",
              " 'f1-score': 0.7836097246820157}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywA1O6tM48xG"
      },
      "source": [
        "## Датасет с символами и словами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RFNA8Sd19d5"
      },
      "source": [
        "class SymTweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['prep_text'].values\n",
        "        self.word2id = word2id\n",
        "        self.sym2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        syms = list(self.dataset[index])\n",
        "        words = word_tokenize(self.dataset[index])\n",
        "        w_ids = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        s_ids = torch.LongTensor([self.sym2id[sym] for sym in syms if sym in self.sym2id])\n",
        "        y = [self.target[index]]\n",
        "        return w_ids, s_ids, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      wids, sids, y = list(zip(*batch))\n",
        "      padded_wids = pad_sequence(wids, batch_first=True).to(self.device)\n",
        "      padded_sids = pad_sequence(sids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_wids, padded_sids, y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nkz6lHu5IVw"
      },
      "source": [
        "train_dataset = SymTweetsDataset(train_data, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "val_dataset = SymTweetsDataset(val_data, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceKKv3nx8J37"
      },
      "source": [
        "## Модель с символами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_-dInf38Q_2"
      },
      "source": [
        "class SCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, word_vocab_size, sym_vocab_size, embedding_dim):\n",
        "        super().__init__()          \n",
        "        self.wembedding = nn.Embedding(word_vocab_size, embedding_dim)\n",
        "        self.wembedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.sembedding = nn.Embedding(sym_vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.emb2h = nn.Linear(100, 10)\n",
        "        self.h2out = nn.Linear(190, 1)  \n",
        "        self.relu = nn.ReLU()\n",
        "        self.act2 = nn.Sigmoid() \n",
        "        \n",
        "        \n",
        "    def forward(self, words, syms):\n",
        "        \n",
        "        embedded = self.wembedding(words)\n",
        "        mean_emb = torch.mean(embedded, dim=1)\n",
        "        word_emb = self.emb2h(mean_emb)\n",
        "        sym_emb = self.sembedding(syms).transpose(1,2)\n",
        "        feature_map_bigrams = self.pooling(self.relu(self.bigrams(sym_emb)))\n",
        "        feature_map_trigrams = self.pooling(self.relu(self.trigrams(sym_emb)))\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        concat = torch.cat((pooling1, pooling2), 1)\n",
        "        concat = torch.cat((concat, word_emb), 1)\n",
        "        out = self.h2out(concat)\n",
        "        proba = self.act2(out)\n",
        "        \n",
        "        return proba"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eddmE93w71WU"
      },
      "source": [
        "def strain(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (ws, sms, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(ws, sms)\n",
        "        loss = criterion(preds, ys)\n",
        "        loss.backward() \n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator)\n",
        "\n",
        "def sevaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (wids, sids, ys) in enumerate(iterator):   \n",
        "            preds = model(wids, sids)\n",
        "            loss = criterion(preds, ys)\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def spredict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (words, syms, ys) in enumerate(iterator):   \n",
        "            preds = model(words, syms)\n",
        "            for pred, gold, word in zip(preds, ys, words):\n",
        "              text = ' '.join([id2word[int(w)] for w in word if w !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    pr = len(tp) / (len(tp) + len(fp))\n",
        "    rc = len(tp) / (len(tp) + len(fn))\n",
        "    f1sc = 2*pr*rc / (pr + rc) \n",
        "    return fp, fn, tp, tn, {'precision': pr, 'recall': rc, 'f1-score': f1sc}"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u8n5spu35_N",
        "outputId": "c7061819-194b-4aa8-90f0-74d9c3a39261"
      },
      "source": [
        "model = SCNN(len(word2id), len(symbol2id), 100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = nn.BCELoss()  \n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = strain(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = sevaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = sevaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7433311939239502\n",
            "Train loss: 0.6594492896743442\n",
            "Train loss: 0.6238732085778163\n",
            "Train loss: 0.599554007001918\n",
            "Train loss: 0.5790315561596004\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5048245340585709, Val f1: 0.8093041777610779\n",
            "Val loss: 0.49336633302163385, Val f1: 0.8009685277938843\n",
            "Val loss: 0.49068238500219125, Val f1: 0.7968689203262329\n",
            "Val loss: 0.48987250533892956, Val f1: 0.794594943523407\n",
            "Val loss: 0.4888004884980191, Val f1: 0.7933446168899536\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5699881017208099, Val f1: 0.8776243329048157\n",
            "Val loss: 0.5395218870219063, Val f1: 0.8237985968589783\n",
            "Val loss: 0.5313879595353053, Val f1: 0.8052682280540466\n",
            "Val loss: 0.5286881770406451, Val f1: 0.7959586977958679\n",
            "Val loss: 0.5242205519567836, Val f1: 0.7918329238891602\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.4769461707157247\n",
            "Train loss: 0.46487372897673346\n",
            "Train loss: 0.4571418621792243\n",
            "Train loss: 0.4525961189818897\n",
            "Train loss: 0.4503116633357673\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39776711691828337, Val f1: 0.8500177264213562\n",
            "Val loss: 0.39325874439184216, Val f1: 0.8379949927330017\n",
            "Val loss: 0.3936592137011198, Val f1: 0.8325353860855103\n",
            "Val loss: 0.39256810863241015, Val f1: 0.8304316401481628\n",
            "Val loss: 0.3925089503841839, Val f1: 0.8283339738845825\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5139600858092308, Val f1: 0.8776939511299133\n",
            "Val loss: 0.48606934617547426, Val f1: 0.8265646696090698\n",
            "Val loss: 0.4779237566085962, Val f1: 0.8081904649734497\n",
            "Val loss: 0.4755646892956325, Val f1: 0.7983992099761963\n",
            "Val loss: 0.4714944301681085, Val f1: 0.7926910519599915\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.40457534264115724\n",
            "Train loss: 0.3987172371235447\n",
            "Train loss: 0.39710395811842036\n",
            "Train loss: 0.3963693873487788\n",
            "Train loss: 0.39703893421710224\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3882729674086851, Val f1: 0.8678824305534363\n",
            "Val loss: 0.38566927028738934, Val f1: 0.8532969355583191\n",
            "Val loss: 0.381340804294898, Val f1: 0.8501546382904053\n",
            "Val loss: 0.38027106000365113, Val f1: 0.8477603197097778\n",
            "Val loss: 0.3787822000596715, Val f1: 0.847308337688446\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5544988550245762, Val f1: 0.8912650942802429\n",
            "Val loss: 0.5214585889788235, Val f1: 0.839343786239624\n",
            "Val loss: 0.5126785681797907, Val f1: 0.8208468556404114\n",
            "Val loss: 0.5110467238085611, Val f1: 0.8113776445388794\n",
            "Val loss: 0.506225551393899, Val f1: 0.8066403269767761\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.37265917658805847\n",
            "Train loss: 0.36490992575451947\n",
            "Train loss: 0.36274751419058215\n",
            "Train loss: 0.36302428387051866\n",
            "Train loss: 0.3633449725035963\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.34139682615504546, Val f1: 0.8730049729347229\n",
            "Val loss: 0.3374262167059857, Val f1: 0.8600344657897949\n",
            "Val loss: 0.3351687826216221, Val f1: 0.856377899646759\n",
            "Val loss: 0.3336610669712369, Val f1: 0.8548570275306702\n",
            "Val loss: 0.3326780081822954, Val f1: 0.8533438444137573\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.55512585490942, Val f1: 0.8493649959564209\n",
            "Val loss: 0.522337019443512, Val f1: 0.8033579587936401\n",
            "Val loss: 0.512668075469824, Val f1: 0.7872713208198547\n",
            "Val loss: 0.5097113047327314, Val f1: 0.77888023853302\n",
            "Val loss: 0.5063829543915662, Val f1: 0.77463299036026\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.33188881768899803\n",
            "Train loss: 0.3325818414273469\n",
            "Train loss: 0.3357237846805499\n",
            "Train loss: 0.33774610088883544\n",
            "Train loss: 0.3404596216719726\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.30871430915944714, Val f1: 0.8977622389793396\n",
            "Val loss: 0.3061436254909073, Val f1: 0.8845549821853638\n",
            "Val loss: 0.3055744870350911, Val f1: 0.8791632652282715\n",
            "Val loss: 0.30493374694165565, Val f1: 0.8772243857383728\n",
            "Val loss: 0.3042544205298369, Val f1: 0.876295268535614\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5626065507531166, Val f1: 0.8649476170539856\n",
            "Val loss: 0.5289268879329457, Val f1: 0.8163493871688843\n",
            "Val loss: 0.5198466628789902, Val f1: 0.7991507053375244\n",
            "Val loss: 0.5171514826161521, Val f1: 0.7919585704803467\n",
            "Val loss: 0.514515754851428, Val f1: 0.7875169515609741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNxMWODU4yFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2b4cc4-fe59-4d6b-f791-ad7f410e33b0"
      },
      "source": [
        "fp, fn, tp, tn, metr = spredict(model, val_iterator)\n",
        "metr"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision': 0.7899903665305748,\n",
              " 'recall': 0.7521072629602131,\n",
              " 'f1-score': 0.7705834974046893}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f0OYDBZqpMp",
        "outputId": "fbffb586-954b-4813-a4df-3c1a4f3e2b60"
      },
      "source": [
        "print(len(fp), len(fn), len(tp), len(tn))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4578 5676 17221 17892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYu7CxBaq3SZ",
        "outputId": "e8f921e7-1867-4be9-da46-74b2940440ac"
      },
      "source": [
        "fp[:20]\n",
        "# не всегда справляется с конфликтами пол. vs отриц. слов (убью vs добрые, адский vs радует)\n",
        "# в некоторых предложениях нет никаких специфичных слов, которые могут помочь, и периодически не хватает контекста, чтобы самому понять окраску\n",
        "# но есть ошибки и со словами \"жалко, боюсь\", то есть негативными"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER стаса друг очень просит таню в принципе она мешать не будет но все равно',\n",
              " 'прохождение the walking dead season 2 девочку жалко 4 URL',\n",
              " 'rt USER я думаю у кое кого есть маленький секрет teamfollowback',\n",
              " 'USER вася ааа как давно я его не видела',\n",
              " 'USER USER какой то тогда если бы я это увидела спокойно не смогла бы уйти от туда',\n",
              " 'они сейчас съедят все сушки как дальше принимать задачки',\n",
              " 'USER USER USER а у нас в украине 2 уже забыл сколько недель восток а в крыму уже цветут деревья и все печально',\n",
              " 'на работе мы внезапно решили заменить сервер на котором домена и прочие ad',\n",
              " 'USER 8 6 в пяти минутах до школы и вечно опаздываю',\n",
              " 'сегодня просто адский день хоть по конституции отчиталась одно радует',\n",
              " 'лиз ты ушла из 62 da URL',\n",
              " 'я бы предложила встретиться в любое удобное время для тебя но ты слишком занятой',\n",
              " 'USER я вас сейчас убью но эти добрые люди барда и смауга',\n",
              " 'USER пиздец как проверь веки руки',\n",
              " 'USER я завтра просто я была сегодня занята с и',\n",
              " 'USER я тоже глик ловатик и люблю джонасов любимые братики почему то я уверен ты умеешь петь ты просто меня не слышал лал',\n",
              " 'USER никогда не сделаем потомучто',\n",
              " 'USER тому уже скоро первый юбилей отмечать будем',\n",
              " 'USER для того чтобы понять мне надо уйму материала перечитать а мозг уже пухнет да еще и на экзамене боюсь в ступор впасть',\n",
              " 'я думал что елка в искусственная ну ладно пока']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXJgKMSiq6HL",
        "outputId": "653d4d64-65d0-4cbd-88bc-46a54717c547"
      },
      "source": [
        "fn[:20]\n",
        "# все еще сохраняется проблема с разметкой\n",
        "# периодически влияют негативно окрашенные слова, которые в данном контексте не имеют негативных коннотаций (расплачусь, мат, жалоба)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['USER с мамой что то случилось обычно мы ее наряжаем прям под нг',\n",
              " 'кристина меня и заботиться на фитнес приглашает вот так вот',\n",
              " 'скоро по геометрии отличницей стану три 5 за последние контрольные',\n",
              " 'USER я не удивлюсь если ты напишешь им жалобу что тебе смс не пришло',\n",
              " 'USER вы как никогда владислав',\n",
              " 'каждый день иди к цели будто и нет',\n",
              " 'нет я решил сделать перерыв и не есть мясного вообще какое то время сосиски ветчина и т д на следующей неделе',\n",
              " 'кто на пацаны из slipknot вродь называются ахуеть шутка',\n",
              " 'думала что я дома одна но нет сестра проснулась',\n",
              " 'бля надо искать платье на нг а мне так лееень',\n",
              " 'USER один мой друг меня на один пункт в этом списке',\n",
              " 'да 3 вечер сериал надо какой нибудь найти и смотреть',\n",
              " 'USER там много плюсов 1 обновление т е когда загрузилась лента тебя не выкидывает в ее верх а остаешься где закончил читать',\n",
              " 'пошла гулять с лучшей подругой все прохожие убегают в нарнию и думают что мы ахаха это про меня и',\n",
              " 'ой ой ой ой ой вы серьезно gta san andreas на моем ipad я сейчас расплачусь от ностальгии URL',\n",
              " 'USER вот если честно то не знаю где то пт',\n",
              " 'USER ретвит в японских так после своего ретвита пишут чтобы его прокомментировать',\n",
              " 'USER а может он потому что летит не из а в люк',\n",
              " 'а у нас в пб теперь поиграть URL',\n",
              " 'гримм жжет видимо каждая серия будет про какую то из сказок в начале серии 2е в чей то дом и']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4sH_ZhVy6oW"
      },
      "source": [
        "## Улучшение моделей: добавление пунктуации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de2XdrnAuTfg",
        "outputId": "676f0637-a031-4f4e-f33b-f23448a3073a"
      },
      "source": [
        "tweets_data['prep_text'] = [text.lower() for text in tweets_data['text']]\n",
        "train_data, val_data = train_test_split(tweets_data, test_size=0.2)\n",
        "word_vocab = Counter()\n",
        "for text in tweets_data['prep_text']:\n",
        "    word_vocab.update(word_tokenize(text))\n",
        "print('всего уникальных символов:', len(word_vocab))\n",
        "\n",
        "filtered_word_vocab = set()\n",
        "\n",
        "for word in word_vocab:\n",
        "    if word_vocab[word] > 2:\n",
        "        filtered_word_vocab.add(word)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_word_vocab))\n",
        "\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_word_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "\n",
        "id2word = {i:word for word, i in word2id.items()}\n",
        "\n",
        "sym_vocab = Counter()\n",
        "for symbol in tweets_data['prep_text']:\n",
        "    sym_vocab.update(list(symbol))\n",
        "print('всего уникальных символов:', len(sym_vocab))\n",
        "\n",
        "filtered_sym_vocab = set()\n",
        "\n",
        "for symbol in sym_vocab:\n",
        "    if sym_vocab[symbol] > 5:\n",
        "        filtered_sym_vocab.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_sym_vocab))\n",
        "\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_sym_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 323972\n",
            "уникальных символов, втретившихся больше 5 раз: 65696\n",
            "всего уникальных символов: 385\n",
            "уникальных символов, втретившихся больше 5 раз: 169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-rKeoMHttvd"
      },
      "source": [
        "train_dataset = SymTweetsDataset(train_data, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "val_dataset = SymTweetsDataset(val_data, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SnpwMcWulGH",
        "outputId": "a53b5dad-0b21-4c10-987e-4e180b71e65d"
      },
      "source": [
        "model = SCNN(len(word2id), len(symbol2id), 100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = nn.BCELoss()  \n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(1):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = strain(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = sevaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = sevaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.21863990953630386\n",
            "Train loss: 0.11201766492340012\n",
            "Train loss: 0.07545812473892091\n",
            "Train loss: 0.05695359065137107\n",
            "Train loss: 0.04567639956544501\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0010686312384410378, Val f1: 1.029268741607666\n",
            "Val loss: 0.0008765000363211215, Val f1: 1.0144083499908447\n",
            "Val loss: 0.0008614762751116919, Val f1: 1.0095295906066895\n",
            "Val loss: 0.000859570981375203, Val f1: 1.0070953369140625\n",
            "Val loss: 0.0009108060196807459, Val f1: 1.0056346654891968\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0010821388823387679, Val f1: 1.1248828172683716\n",
            "Val loss: 0.0009137498461367452, Val f1: 1.0587109327316284\n",
            "Val loss: 0.0008506488677364989, Val f1: 1.0383878946304321\n",
            "Val loss: 0.000836358210238229, Val f1: 1.028489112854004\n",
            "Val loss: 0.0008160135079048235, Val f1: 1.0226397514343262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9ESpqY0utVh",
        "outputId": "8ed76549-fc66-4f35-d8ff-037cf4dbba6a"
      },
      "source": [
        "fp, fn, tp, tn, metr = spredict(model, val_iterator)\n",
        "metr"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision': 0.9998262606958259,\n",
              " 'recall': 1.0,\n",
              " 'f1-score': 0.9999131228009209}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q95yYZHDxhNV",
        "outputId": "e0e99081-218f-4399-b6a3-fbdf676e6be8"
      },
      "source": [
        "print(len(fp), len(fn), len(tp), len(tn))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 0 23019 22344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nfr_R5WxoxE",
        "outputId": "0ee18766-9266-4ff8-c4c8-6c3992327f3a"
      },
      "source": [
        "fp\n",
        "# Непонятные твиты, которые еще и непонятно почему негативные\n",
        "# Да здравствуют смайлики!"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['то чувство , когда твой лайк 6666 о_о http :',\n",
              " \"заза , сайхан , `` ажил '' ээ , хэх… : -/ # баас # : p\",\n",
              " 'rt @ palmjuliefrost : 4 000 * -- -- -* посвящение : @ @ @ @ @ imapotato_ouo я вас люблю',\n",
              " 'мама : - , ты не проспал я : - я еще не ложился : |']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpG69wOkrncc",
        "outputId": "0e8ff817-11e3-4eac-8be9-e3500feec70f"
      },
      "source": [
        "#Улучшение 1 модели\n",
        "train_dataset = TweetsDataset(train_data, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "val_dataset = TweetsDataset(val_data, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)\n",
        "\n",
        "model = CNN(len(word2id), 100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(1):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.18916034758748376\n",
            "Train loss: 0.10254128946774248\n",
            "Train loss: 0.07170929650041777\n",
            "Train loss: 0.05557144834432188\n",
            "Train loss: 0.045980331604368985\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.005724139415713794, Val f1: 1.027681589126587\n",
            "Val loss: 0.005513945418963398, Val f1: 1.0127688646316528\n",
            "Val loss: 0.005470135612995364, Val f1: 1.007906436920166\n",
            "Val loss: 0.005322808749042451, Val f1: 1.0055369138717651\n",
            "Val loss: 0.0052769365044706765, Val f1: 1.0040948390960693\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.00860236119478941, Val f1: 1.1219595670700073\n",
            "Val loss: 0.006826780640574939, Val f1: 1.056486964225769\n",
            "Val loss: 0.006650491017633333, Val f1: 1.0362749099731445\n",
            "Val loss: 0.006798500388062426, Val f1: 1.0263086557388306\n",
            "Val loss: 0.006975630663377656, Val f1: 1.0204250812530518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erKIwlfdmj71",
        "outputId": "46ab162c-d4e3-49a5-9c6a-480a0a542adf"
      },
      "source": [
        "fp, fn, tp, tn, metr = predict(model, val_iterator)\n",
        "print(metr)\n",
        "print(len(fp), len(fn), len(tp), len(tn))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': 0.9986073635651492, 'recall': 0.9968287067205353, 'f1-score': 0.997717242428854}\n",
            "32 73 22946 22316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbhKSR0Rmst5",
        "outputId": "227238b0-6f91-47dd-9e20-be1502606782"
      },
      "source": [
        "fp\n",
        "#опять разметка вызывает вопросы\n",
        "# иногда обманки-позитивные слова (1 пр.)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@ veeetch всё отлично , если не считать того что я заболела и у меня голос пропадает ( ихихих слышал бы ты меня : d',\n",
              " 'rt @ tao_oppa : я нашел свою папку с оо http :',\n",
              " 'внезапно вспомнила , что мне сегодня снился о_о там и другие , но его прям хорошо помню :3',\n",
              " 'rt @ : @ seo_optimizator вот где можно заработать в интернете https :',\n",
              " '@ , охренеть ! у меня вообще слов нет ! o_o а мы тут радуемся 6 : -|',\n",
              " '@ samo_obayaniye это ты так думаешь , а слог думает иначе',\n",
              " '@ mihaylenko_o знакомо многим , у кого есть младшие твари тот нам надо .',\n",
              " '# rt но мы остались в больнице с пятницы по воскресенье o_o # ff # follow',\n",
              " 'разговор на работе : с : я за праздники набрала 6 кг ! л : а я 3,5 я : а я 1. c , л : o_o fuck yeah !',\n",
              " '@ можно ставить галочку в : /',\n",
              " 'ничего не купила себе из одежды ( вместо этого везу ковер и кучу себе в общагу : d',\n",
              " '( лазерный hp цвет : black ресурс : стр',\n",
              " 'rt @ radio_of_moon : модные достаточно оригинальная и стильная обувь http :',\n",
              " '@ blo_odred моде попробовать записать разговор и в ?',\n",
              " '@ sherlock_go_on я даже фильм из-за них не могу посмотреть d :',\n",
              " '( например , можно проводить соревнования используя при стрельбе лежа для стрельбы из положения стоя и т',\n",
              " 'пересматривал десятого доктора и увидел ... капальди ! o_o интересно , как сценаристы это : / # докторкто',\n",
              " '@ simonenko_os я тоже тебе это обещаю :',\n",
              " 'rt @ angelino_ochka : холод , иди нахуй . снег , иди нахуй . зима , иди нахуй .',\n",
              " 'rt @ do_or_pie_ : @ спасибо с : : # срождествомтвиттерский',\n",
              " 'rt @ palmjuliefrost : 4 000 * -- -- -* посвящение : @ @ @ @ @ imapotato_ouo я вас люблю',\n",
              " '@ loscut1958 @ ivanandhispike @ cio_optimal @ ssukat статистика ваша , : 146 % за путина !',\n",
              " '@ trubadur33 @ cio_optimal а это уже не наши деньги . вы что считаете живет на налоги населения ? нет это качает и бросает нам кость',\n",
              " '@ да я тоже еле уговорила её дать мне поносить на 5 мин азахха',\n",
              " 'а : да давай скажем ей ... ж : что ? а : куда сергей смотрел весь день ... ж : o_o , не надо мне говорить ... ж : вот же суки ...',\n",
              " 'rt @ mamo_oru : любимые мужские имена : александр . # эгоизм .',\n",
              " 'rt @ lavrovalove : @ sveta_o_o эта девочка самая лучшая ❤ люблю',\n",
              " '@ panfilenko_olga легко и просто , я так тоже делаю ; d',\n",
              " '@ elenamikhailova @ cio_optimal свои же и режут из-за денег .',\n",
              " 'rt @ horanso_on : найл : рок ме гарри : меня штырит зейн : о , драчка лиам : ска , почему ты мне сказал , что джинсы надо черные луи : все норм http : …',\n",
              " 'rt @ : @ to_over_kill потому что зебры крутые .',\n",
              " '@ o_o что это там за фотки , и я такие хочу ; d']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2N6-wvZmvd3",
        "outputId": "47ab8c0e-89f0-429a-fc28-2a1ddb6a341b"
      },
      "source": [
        "fn[:20]\n",
        "# возможно, считает ^_^ и большое кол-во ! негативным\n",
        "# хотя попалось достаточно много примеров со словами с очень ярко-выраженным позитивным окрасом"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt @ : @ love_niall99 не боись , всегда и всюду с тобой : *удачи , лю типя ! ! !',\n",
              " '@ : . сегодня фильм в дк есть ? да ?',\n",
              " 'rt @ : @ с рождения тебя ! ! успехов во всех начинаниях , здоровья , оставайся http : //…',\n",
              " 'у любимой моей глазки http :',\n",
              " 'rt @ : @ im_your_maniac : dddd',\n",
              " 'rt @ : наверное , стоит дать им минутку побыть вдвоём , перед операцией . *^_^* # teamfollowback',\n",
              " 'украсили комнату и нарядили ёлочку =^_^= http :',\n",
              " '@ lirvana_ @ fucking_chaw @ killjoy_greenka @ 666_idiot @ deatbeatholiday @ fuck_time01 ну гоу гоу : вы там охранников не , а то',\n",
              " 'rt @ : @ ты всегда лапочка',\n",
              " '@ адрес адрес адрес ! ! ты сказал мне адрес ! ! ! а теперь будь осторожнее ! ! :',\n",
              " 'думаю стоит поздравить одну из самых @ с днем рождения:3 желаю не заболеть в и пошире глотку',\n",
              " 'круто , зато на химию не ходила : не нравится мне этот предмет',\n",
              " '@ @ khatsevich_ann ^_^ надежда , рб , г. минск , ул . , , кв . , индекс .',\n",
              " ') @ область http :',\n",
              " '@ я сказала свои нет : половина левые вообще были',\n",
              " 'после школы зашли с аней в я накормила нас и :',\n",
              " 'сдала теорию ! ! ! ! с первого ! ! ! ! ! кто тут самый умный ? ! ? ! ?',\n",
              " '@ god_of_cuteness @ enot_boris за всех нас с : за эту прекрасную встречу : *****',\n",
              " '# / , ходи по чаще в рубашке , она тебе',\n",
              " '@ что ты делаешь , любовь моя ? тоже безумно скучаю :']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANt4oPpKx59O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}