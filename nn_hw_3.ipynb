{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Способы выделения продуктов категории CDs&Vinyl:\n",
    "\n",
    "1. Rule-based модель spacy (реализована). Из метаинформации понадобятся названия продуктов. Также я брала токены cd, album, disc, vinyl, record, LP и их сочетания со словами, начинающимися с заглавных букв.\n",
    "\n",
    "2. Составить словарь сущностей (можно взять из прошлого пункта). Потом расширить его с помощью эмбеддингов, посмотрев ближайшие к ним. \n",
    "\n",
    "3. Можно попробовать сделать классификатор, который будет предсказывать категорию продуктов, и модель, которая будет предсказывать названия конкретных продуктов. Потом взять слова, у которых будут наибольшие веса, первая модель должна выдать что-то в духе cd, album и т.д., вторая - названия (в отличие от 1 пункта, здесь легче должны считываться неполные названия конкретных продуктов)\n",
    "\n",
    "4. Вариант трудоемкий: сначала разметить часть данных вручную/через ruled-based модель, потом обучить на этих данных модель из allenlp, например. \n",
    "\n",
    "### Реализация 1 варианта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9BGAfRKG4Be5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, bigrams\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics.spearman import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание датафреймов с отзывами и метаинформацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "swf-bClG4Be6"
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_CDs_and_Vinyl_5.json.gz')\n",
    "meta = getDF('meta_CDs_and_Vinyl.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание паттернов, извлечение наваний продуктов и биграм с ними (брала только первые 100000 отзывов, про многих нет метаинформации, поэтому всего получился корпус из 46745 отзывов).\n",
    "\n",
    "Паттерны:\n",
    "\n",
    "1) Названия продуктов\n",
    "\n",
    "2) cd/album/vinyl/disc/LP/record + (опционально) ('/\") Слова_с_заглавной_буквы ('/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbINBzYexWjY",
    "outputId": "df56dbf7-e739-428e-f88a-f5d4d338409e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3028/3028 [09:56<00:00,  5.08it/s]\n"
     ]
    }
   ],
   "source": [
    "patterns = [[{'LOWER': 'cd'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}, {'IS_TITLE': 'True', 'OP': '*'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}], [{'LOWER': 'album'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}, {'IS_TITLE': 'True', 'OP': '*'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}], [{'LOWER': 'disc'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}, {'IS_TITLE': 'True', 'OP': '*'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}], [{'LOWER': 'vinyl'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}, {'IS_TITLE': 'True', 'OP': '*'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}], [{'LOWER': 'record'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}, {'IS_TITLE': 'True', 'OP': '*'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}], [{'TEXT': 'LP'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}, {'IS_TITLE': 'True', 'OP': '*'}, {'TEXT': {'REGEX': '\"|\\''}, 'OP': '?'}]]\n",
    "terms = []\n",
    "rws = []\n",
    "for id_p in tqdm(set(df['asin'][:100000]) & set(meta['asin'])):\n",
    "    tt = meta.loc[meta['asin'] == id_p]['title'].tolist()[0]\n",
    "    rw = df.loc[df['asin'] == id_p]['reviewText'].tolist()\n",
    "    rws.extend(rw)\n",
    "    terms.append(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uFQ2RifTnkB_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3028/3028 [00:06<00:00, 447.46it/s]\n",
      "100%|██████████| 46745/46745 [08:38<00:00, 90.09it/s] \n",
      "100%|██████████| 46745/46745 [01:32<00:00, 507.24it/s] \n",
      "100%|██████████| 46745/46745 [00:16<00:00, 2770.04it/s] \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ph_matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "ph_patterns = [nlp.make_doc(text) for text in tqdm(terms)]\n",
    "ph_matcher.add('PRD', ph_patterns)\n",
    "matcher.add('PRD', patterns)\n",
    "docs = [nlp.make_doc(text) for text in tqdm(rws)]\n",
    "matches = [matcher(doc) for doc in tqdm(docs)]\n",
    "ph_matches = [ph_matcher(doc) for doc in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "results = []\n",
    "for match, ph_match, doc in zip(matches, ph_matches, docs):\n",
    "    res = []\n",
    "    for match_id, start, end in match:\n",
    "        span = doc[start:end]\n",
    "        res.append(span.text)\n",
    "        if start > 0:\n",
    "            ngrams.append((doc[start-1].text, span.text))\n",
    "        if end < len(doc) - 1:\n",
    "            ngrams.append((span.text, doc[end+1].text))\n",
    "    for match_id, start, end in ph_match:\n",
    "        span = doc[start:end]\n",
    "        res.append(span.text)\n",
    "        if start > 0:\n",
    "            ngrams.append((doc[start-1].text, span.text))\n",
    "        if end < len(doc) - 1:\n",
    "            ngrams.append((span.text, doc[end+1].text))\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46745it [22:48, 34.16it/s] \n"
     ]
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "words = []\n",
    "ngs = []\n",
    "for rw, r, ng in tqdm(zip(rws, results, ngrams)):\n",
    "    ws = word_tokenize(rw)\n",
    "    words.extend(ws)\n",
    "    words.extend([w for w in r if w not in ws])\n",
    "    ngs.extend(list(bigrams(ws)))\n",
    "    ngs.extend([n for n in ng if n not in ngs])\n",
    "w_fd = FreqDist(words)\n",
    "bg_fd = FreqDist(ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder(w_fd, bg_fd)\n",
    "finder.apply_freq_filter(3)\n",
    "nes = []\n",
    "for r in results:\n",
    "    nes.extend(r)\n",
    "nes = set(nes)\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_ngram_filter(lambda w1, w2: (w1 not in nes and w2 not in nes) or w1 in sw or w2 in sw or w1.isalpha() == False or w2.isalpha() == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_pmi = finder.score_ngrams(bigram_measures.pmi)\n",
    "sc_t = finder.score_ngrams(bigram_measures.student_t)\n",
    "sc_ll = finder.score_ngrams(bigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ranks_from_scores(sc_pmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ranks_from_scores(sc_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ranks_from_scores(sc_ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_groups = {}\n",
    "for n, r in list(ranks_from_scores(sc_t)):\n",
    "    for w in n:\n",
    "        if w in nes:\n",
    "            if w not in ne_groups:\n",
    "                ne_groups[w] = [(n[0] + ' ' + n[1], r)]\n",
    "            else:\n",
    "                ne_groups[w].append((n[0] + ' ' + n[1], r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_groups['album'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_groups['cd'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_groups['disc'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_groups['vinyl'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dvd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-4e802a397d18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mne_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dvd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'dvd'"
     ]
    }
   ],
   "source": [
    "ne_groups['set'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Борьба с синонимами:\n",
    "\n",
    "1) Можно попробовать воспользоваться синсетами из wordnet или его аналогов\n",
    "\n",
    "2) Также часто можно объединять однокоренные слова/ н-грамм с общей вершиной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
