{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y98GLxJxNife",
        "outputId": "18959ccd-bed9-43f6-d893-c3b01b59a49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.functional import f1\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68hRRtNHOAEg",
        "outputId": "3104d70a-b6b8-4713-ee14-bee275936f12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-29 17:12:51--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.228.75\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.228.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.2MB/s    in 1m 41s  \n",
            "\n",
            "2021-12-29 17:14:32 (15.6 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "trXkuqUjSZ2R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HjhmNkJqNifh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Fake.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Препроцессинг - в статье только приведение к нижнему регистру + паддинг, я добавила очистку от пунктуации, поскольку у нас не задача оценки тональности, и смайлики и прочее вряд ли сможет многое сказать о теме. И еще немного изменила паддинг (в оригинале паддилось до максимальной длины текста в трейне, но у нас макс. длина около 7800, а большая часть текстов длиной не превышает 512, поэтому буду обрезать и паддить до нее)"
      ],
      "metadata": {
        "id": "52AXLhvWuwK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "ls = []\n",
        "cat_mapper = {cat: n for n, cat in enumerate(df.subject.unique())}\n",
        "df.subject = df.subject.map(cat_mapper)\n",
        "train_data, val_data = train_test_split(df, test_size=0.2, stratify=df.subject.values)\n",
        "word_vocab = Counter()\n",
        "for text in tqdm(df['text']):\n",
        "    text = preprocess_text(text)\n",
        "    word_vocab.update(text)\n",
        "    #ls.append(len(text))\n",
        "    \n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in word_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "metadata": {
        "id": "cIKodiSPQ8jO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582c6813-1dda-46b9-d49c-6c6a072a3801"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23481/23481 [01:13<00:00, 317.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_mapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG6vPZayW9He",
        "outputId": "a4a5f62b-8018-4af3-febe-af777b7bb989"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Government News': 2,\n",
              " 'Middle-east': 5,\n",
              " 'News': 0,\n",
              " 'US_News': 4,\n",
              " 'left-news': 3,\n",
              " 'politics': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns_plot = sns.distplot(ls)\n",
        "fig = sns_plot.get_figure()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "4df4e2e7-0243-4de3-f393-52ca09e0544b",
        "id": "57kxpOjyOCSK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ls), max(ls))\n",
        "print(len([l for l in ls if l <= 256]))\n",
        "print(len([l for l in ls if l <= 512]))\n",
        "print(len([l for l in ls if l <= 1024]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8DoTgO1cr2l",
        "outputId": "cdd20707-047e-4973-b5ab-66c7903a5c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23481 7815\n",
            "6753\n",
            "18325\n",
            "22567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['subject'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = preprocess_text(self.dataset[index])\n",
        "        ids = [self.word2id[word] for word in words if word in self.word2id][:512]\n",
        "        ids = F.pad(torch.LongTensor(ids), pad=(0, 512 - len(ids)))\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = torch.LongTensor(len(ids), 512)\n",
        "      for i, id in enumerate(ids):\n",
        "          padded_ids[i] = id\n",
        "      y = torch.LongTensor(y).to(self.device)\n",
        "      return padded_ids.to(self.device), y"
      ],
      "metadata": {
        "id": "FouWPEXWSm94"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NewsDataset(train_data, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=64)\n",
        "val_dataset = NewsDataset(val_data, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=64)"
      ],
      "metadata": {
        "id": "Fe_JNRd3TNLe"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVPNXxRQNifh",
        "outputId": "99e83332-e8b9-4f9d-ed29-5877ca6fb653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "weights = np.zeros((len(word2id), 300))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "        count += 1\n",
        "        weights[i] = np.random.normal(-0.25,0.25,300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Mu7nYb4_Nifi"
      },
      "outputs": [],
      "source": [
        "class CLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, cnn_n=[3], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 300)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=False)\n",
        "        self.cnn_n = cnn_n\n",
        "        if 2 in cnn_n:\n",
        "            self.bigrams = nn.Conv1d(in_channels=300, out_channels=cnn_oc[cnn_n.index(2)], kernel_size=2, padding='valid')\n",
        "        if 3 in cnn_n:\n",
        "            self.trigrams = nn.Conv1d(in_channels=300, out_channels=cnn_oc[cnn_n.index(3)], kernel_size=3, padding='valid')\n",
        "        if 4 in cnn_n:\n",
        "            self.qgrams = nn.Conv1d(in_channels=300, out_channels=cnn_oc[cnn_n.index(4)], kernel_size=4, padding='valid')\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lstm = nn.LSTM(input_size=sum(cnn_oc), hidden_size=lstm_dim, num_layers=1, batch_first=True)\n",
        "        self.dp_l = dp_l\n",
        "        if dp_l !=0:\n",
        "            self.dp = nn.Dropout(p=dp_p)\n",
        "        self.linear = nn.Linear(lstm_dim, 6)\n",
        "\n",
        "    def forward(self, n):\n",
        "        embedded = self.embedding(n)\n",
        "        if self.dp_l == 1:\n",
        "            embedded = self.dp(embedded)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_ngrams = []\n",
        "        if 2 in self.cnn_n:\n",
        "            feature_map_ngrams.append(self.relu(self.bigrams(embedded))[:,:,:511 - max(self.cnn_n)])\n",
        "        if 3 in self.cnn_n:\n",
        "            feature_map_ngrams.append(self.relu(self.trigrams(embedded))[:,:,:511 - max(self.cnn_n)])\n",
        "        if 4 in self.cnn_n:\n",
        "            feature_map_ngrams.append(self.relu(self.qgrams(embedded))[:,:,:511 - max(self.cnn_n)])\n",
        "        if len(self.cnn_n) > 1:\n",
        "            feature_map_ngrams = torch.cat(feature_map_ngrams, 1)\n",
        "        else:\n",
        "            feature_map_ngrams = feature_map_ngrams[0]\n",
        "        out, (ht, ct) = self.lstm(feature_map_ngrams.transpose(1,2))\n",
        "        if self.dp_l == 2:\n",
        "            out = self.dp(ht[-1])\n",
        "        out = self.linear(ht[-1])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "kKszpkP5Nifl"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(texts)\n",
        "        loss = criterion(preds, ys.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)\n",
        "            loss = criterion(preds,ys.squeeze())\n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds, ys.squeeze(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(epochs, train_iterator, val_iterator, ps, wd=0):\n",
        "\n",
        "    model = CLSTM(len(word2id), **ps).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    optimizer = optim.RMSprop(model.parameters(), weight_decay=wd)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "    for i in tqdm(range(epochs)):\n",
        "        print(f\"\\nParameters: cnn_n={ps['cnn_n']}, cnn_oc={ps['cnn_oc']}, lstm_dim={ps['lstm_dim']}, dp_l={ps['dp_l']}, dp_p={ps['dp_p']}, wd={wd}\")\n",
        "        print(f'\\nstarting Epoch {i}')\n",
        "        print('Training...')\n",
        "        epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "        losses.append(epoch_loss)\n",
        "        print('\\nEvaluating on train...')\n",
        "        f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "        f1s.append(f1_on_train)\n",
        "        print('\\nEvaluating on test...')\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "    return [losses, losses_eval, f1s, f1s_eval, ps, wd]"
      ],
      "metadata": {
        "id": "0OrTy7PmsOaE"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Эксперименты\n",
        "\n",
        "Dropout\n",
        "\n",
        "1) dp_p = 0.5 - слишком большой"
      ],
      "metadata": {
        "id": "Y69CW2uiwnJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "ps = {'cnn_n': [3], 'cnn_oc':[150], 'lstm_dim':150, 'dp_l':1, 'dp_p':0.5}\n",
        "res = []\n",
        "res.append(get_results(2, train_iterator, val_iterator, ps, wd=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZIeP4uotgd9",
        "outputId": "39cafdf6-2012-4c0b-87db-6520869b8721"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[3], cnn_oc=[150], lstm_dim=150, dp_l=1, dp_p=0.5, wd=0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.8231793391077142\n",
            "Train loss: 1.6641193472820779\n",
            "Train loss: 1.6130632175875537\n",
            "Train loss: 1.593395657353587\n",
            "Train loss: 1.5747657832802373\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.5584880845588551, Val f1: 0.0008996851393021643\n",
            "Val loss: 1.5411185824352762, Val f1: 0.000786936841905117\n",
            "Val loss: 1.5328051141231736, Val f1: 0.0007983629475347698\n",
            "Val loss: 1.5361631375886662, Val f1: 0.0008199083968065679\n",
            "Val loss: 1.5386751560191383, Val f1: 0.0008423975668847561\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.6881263256072998, Val f1: 0.0037523454520851374\n",
            "Val loss: 1.59821946532638, Val f1: 0.0018066847696900368\n",
            "Val loss: 1.5804642962246407, Val f1: 0.003773264354094863\n",
            "Val loss: 1.561386589570479, Val f1: 0.0028127969708293676\n",
            "Val loss: 1.556981995485831, Val f1: 0.0022420845925807953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [03:47<03:47, 227.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[3], cnn_oc=[150], lstm_dim=150, dp_l=1, dp_p=0.5, wd=0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.5413716257664196\n",
            "Train loss: 1.5220178904740707\n",
            "Train loss: 1.521001260404642\n",
            "Train loss: 1.5148339968223077\n",
            "Train loss: 1.5195495063458346\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.5541581166417975, Val f1: 0.0\n",
            "Val loss: 1.538400241603022, Val f1: 0.0\n",
            "Val loss: 1.534130353459044, Val f1: 0.0\n",
            "Val loss: 1.5312199159102007, Val f1: 0.0\n",
            "Val loss: 1.5254703677649317, Val f1: 0.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.665473195222708, Val f1: 0.0\n",
            "Val loss: 1.5781878851078175, Val f1: 0.0\n",
            "Val loss: 1.5614723984788104, Val f1: 0.0\n",
            "Val loss: 1.547856348211115, Val f1: 0.0\n",
            "Val loss: 1.5445563758628955, Val f1: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [07:27<00:00, 223.66s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Без дропаута наилучший размер фильтра - 2 (если мы берем только 1)."
      ],
      "metadata": {
        "id": "NvSAx-4u0E-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "params = list(product(*[[[2], [3], [4]], [[150]], [150], [0], [0]]))\n",
        "for p in params:\n",
        "    ps = {'cnn_n': p[0], 'cnn_oc':p[1], 'lstm_dim':p[2], 'dp_l':p[3], 'dp_p':p[4]}\n",
        "    res.append(get_results(2, train_iterator, val_iterator, ps, wd=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NUDHWQBwgp1",
        "outputId": "28e0e8f1-694c-48c8-af98-af595964532a"
      },
      "execution_count": 128,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.6204347673215365\n",
            "Train loss: 1.4307504622832588\n",
            "Train loss: 1.3494238440011967\n",
            "Train loss: 1.3005870882566872\n",
            "Train loss: 1.2634515015724208\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.1571627321996187, Val f1: 0.44730624556541443\n",
            "Val loss: 1.1634878272595612, Val f1: 0.4409838914871216\n",
            "Val loss: 1.1697335015831656, Val f1: 0.43651482462882996\n",
            "Val loss: 1.1695432851324865, Val f1: 0.4370649456977844\n",
            "Val loss: 1.1705301581369552, Val f1: 0.436114102602005\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.276118516921997, Val f1: 0.4792642295360565\n",
            "Val loss: 1.225641617068538, Val f1: 0.45965734124183655\n",
            "Val loss: 1.2141798691051762, Val f1: 0.4461507797241211\n",
            "Val loss: 1.2000760089267384, Val f1: 0.4447583258152008\n",
            "Val loss: 1.1947462316872417, Val f1: 0.4359973967075348\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [03:50<03:50, 230.17s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.136707395838018\n",
            "Train loss: 1.1044035217036372\n",
            "Train loss: 1.0963394817589336\n",
            "Train loss: 1.0880088380404882\n",
            "Train loss: 1.0840885131829339\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.1702074826809399, Val f1: 0.41323551535606384\n",
            "Val loss: 1.1402857288070347, Val f1: 0.4184524714946747\n",
            "Val loss: 1.1285957559684798, Val f1: 0.42537400126457214\n",
            "Val loss: 1.1207973257803814, Val f1: 0.42713502049446106\n",
            "Val loss: 1.1222960282774532, Val f1: 0.4241015911102295\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2449905092899616, Val f1: 0.48826950788497925\n",
            "Val loss: 1.185607428903933, Val f1: 0.45434412360191345\n",
            "Val loss: 1.1737534723630765, Val f1: 0.43727248907089233\n",
            "Val loss: 1.165165254202756, Val f1: 0.4304657280445099\n",
            "Val loss: 1.1625471391539643, Val f1: 0.4219287037849426\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [07:08<00:00, 214.48s/it]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters: cnn_n=[3], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.783151243862353\n",
            "Train loss: 1.6402744749318\n",
            "Train loss: 1.5981140846461919\n",
            "Train loss: 1.5743401205384886\n",
            "Train loss: 1.5569198148060834\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.5117364034318088, Val f1: 0.0\n",
            "Val loss: 1.504624585483385, Val f1: 0.0004830917459912598\n",
            "Val loss: 1.490907930225306, Val f1: 0.00032113035558722913\n",
            "Val loss: 1.487953724799218, Val f1: 0.00024050023057498038\n",
            "Val loss: 1.4833947890357575, Val f1: 0.00019223375420551747\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.6210755018087535, Val f1: 0.0\n",
            "Val loss: 1.5397642100298847, Val f1: 0.0018518517026677728\n",
            "Val loss: 1.523275901631611, Val f1: 0.0012195120798423886\n",
            "Val loss: 1.504959039254622, Val f1: 0.0009090907988138497\n",
            "Val loss: 1.5014228250669397, Val f1: 0.0007246376480907202\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [03:26<03:26, 206.75s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters: cnn_n=[3], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.5234233329170628\n",
            "Train loss: 1.50968512452167\n",
            "Train loss: 1.5098171337491515\n",
            "Train loss: 1.5042574658538357\n",
            "Train loss: 1.4992981406644141\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.8845572178823906, Val f1: 0.3677920997142792\n",
            "Val loss: 1.8714682040007218, Val f1: 0.3639366924762726\n",
            "Val loss: 1.8620321075351252, Val f1: 0.3634633719921112\n",
            "Val loss: 1.8590424850389555, Val f1: 0.36378660798072815\n",
            "Val loss: 1.8625789653883673, Val f1: 0.36166277527809143\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.9703253507614136, Val f1: 0.40397459268569946\n",
            "Val loss: 1.9021951269220423, Val f1: 0.38608163595199585\n",
            "Val loss: 1.9000613776648916, Val f1: 0.37423545122146606\n",
            "Val loss: 1.8734228112480857, Val f1: 0.3723181486129761\n",
            "Val loss: 1.8841901378355164, Val f1: 0.36380335688591003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [06:46<00:00, 203.47s/it]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters: cnn_n=[4], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.7435541299351476\n",
            "Train loss: 1.6224083216293999\n",
            "Train loss: 1.572988622450415\n",
            "Train loss: 1.5510893136391908\n",
            "Train loss: 1.5416235932016868\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.5865635244469893, Val f1: 0.370564728975296\n",
            "Val loss: 1.5817232100859933, Val f1: 0.3628492057323456\n",
            "Val loss: 1.5762792198644209, Val f1: 0.3632850646972656\n",
            "Val loss: 1.573692452339899, Val f1: 0.36020609736442566\n",
            "Val loss: 1.5723402029915252, Val f1: 0.36076247692108154\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.6828917173238902, Val f1: 0.40397459268569946\n",
            "Val loss: 1.622062572726497, Val f1: 0.38608163595199585\n",
            "Val loss: 1.6066750491537698, Val f1: 0.37423545122146606\n",
            "Val loss: 1.5926725084131415, Val f1: 0.3723181486129761\n",
            "Val loss: 1.5937307122824849, Val f1: 0.36380335688591003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [03:36<03:36, 216.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[4], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.4979146405270225\n",
            "Train loss: 1.5064280582510907\n",
            "Train loss: 1.5046119855318456\n",
            "Train loss: 1.4989593921801745\n",
            "Train loss: 1.4932330879785611\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.5163015670943678, Val f1: 0.06286787241697311\n",
            "Val loss: 1.5077804959338643, Val f1: 0.06238861382007599\n",
            "Val loss: 1.5015716883488472, Val f1: 0.06039377301931381\n",
            "Val loss: 1.4997474048044774, Val f1: 0.06187602877616882\n",
            "Val loss: 1.4973584029913773, Val f1: 0.06356290727853775\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.6028246329380915, Val f1: 0.07255629450082779\n",
            "Val loss: 1.543387527819033, Val f1: 0.06897188723087311\n",
            "Val loss: 1.5316799297565367, Val f1: 0.07033832371234894\n",
            "Val loss: 1.5183872504667801, Val f1: 0.06902037560939789\n",
            "Val loss: 1.5172853348911672, Val f1: 0.06750103831291199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [07:13<00:00, 216.55s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#смотрим на наилучший f1 для каждого набора параметров, потому что модель после 1 эпохи результат может быть лучше, чем после 2\n",
        "print(max(res, key=lambda x: max(x[3]))[-2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn8vLi6O3PzH",
        "outputId": "42104069-46b0-4225-ab7c-5d12584b6476"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим дропаут 0.2 для фильтра размера 2 и посмотрим, к какому слою лучше его добавить (или лучше вообще не добавлять)"
      ],
      "metadata": {
        "id": "b3WYKtYX55Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(product(*[[[2]], [[150]], [150], [1, 2], [0.2]]))\n",
        "for p in params:\n",
        "    ps = {'cnn_n': p[0], 'cnn_oc':p[1], 'lstm_dim':p[2], 'dp_l':p[3], 'dp_p':p[4]}\n",
        "    res.append(get_results(2, train_iterator, val_iterator, ps, wd=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmLVAV-D4avu",
        "outputId": "0608d01e-5043-4b5a-b0b6-fb9689ca5a61"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=1, dp_p=0.2, wd=0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.7611762410716008\n",
            "Train loss: 1.6265043787334277\n",
            "Train loss: 1.5965843069760097\n",
            "Train loss: 1.5682155420253803\n",
            "Train loss: 1.5559816046982076\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.678742998524716, Val f1: 0.36492517590522766\n",
            "Val loss: 1.6630892929823502, Val f1: 0.36542031168937683\n",
            "Val loss: 1.6640241297683276, Val f1: 0.36316436529159546\n",
            "Val loss: 1.6649450698456207, Val f1: 0.3618893623352051\n",
            "Val loss: 1.6631632936042073, Val f1: 0.3615424335002899\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.7971448348118708, Val f1: 0.40397459268569946\n",
            "Val loss: 1.71622305004685, Val f1: 0.38608163595199585\n",
            "Val loss: 1.701114660356103, Val f1: 0.37423545122146606\n",
            "Val loss: 1.6852057521993464, Val f1: 0.3723181486129761\n",
            "Val loss: 1.6856595018635625, Val f1: 0.36380335688591003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [03:49<03:49, 229.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=1, dp_p=0.2, wd=0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.5308623125678615\n",
            "Train loss: 1.5246384019437043\n",
            "Train loss: 1.5192750609679029\n",
            "Train loss: 1.507455243176712\n",
            "Train loss: 1.5075171975528492\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.514417566751179, Val f1: 0.3607430160045624\n",
            "Val loss: 1.4955831081970878, Val f1: 0.3630949556827545\n",
            "Val loss: 1.4874246630365449, Val f1: 0.3662201166152954\n",
            "Val loss: 1.4876768774800486, Val f1: 0.3625134825706482\n",
            "Val loss: 1.4879205024778637, Val f1: 0.36003273725509644\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.6062140556482167, Val f1: 0.40416419506073\n",
            "Val loss: 1.535367418218542, Val f1: 0.38505756855010986\n",
            "Val loss: 1.5224152628968401, Val f1: 0.37239423394203186\n",
            "Val loss: 1.5075644167986784, Val f1: 0.36989179253578186\n",
            "Val loss: 1.505488257477249, Val f1: 0.361837238073349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [07:24<00:00, 222.22s/it]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=2, dp_p=0.2, wd=0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.7804337070699323\n",
            "Train loss: 1.6261687237283458\n",
            "Train loss: 1.5854739033417895\n",
            "Train loss: 1.5694492167724676\n",
            "Train loss: 1.5577265869787407\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.5606246642899095, Val f1: 0.3743458390235901\n",
            "Val loss: 1.5461840049080227, Val f1: 0.3602447807788849\n",
            "Val loss: 1.5383370818430289, Val f1: 0.359679251909256\n",
            "Val loss: 1.5377224136740615, Val f1: 0.35815373063087463\n",
            "Val loss: 1.530683663889611, Val f1: 0.36168065667152405\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.643444611476018, Val f1: 0.40307503938674927\n",
            "Val loss: 1.5696045645961054, Val f1: 0.3850324749946594\n",
            "Val loss: 1.5656444939171397, Val f1: 0.3731420338153839\n",
            "Val loss: 1.544298687848178, Val f1: 0.3715030550956726\n",
            "Val loss: 1.5487075197523918, Val f1: 0.3631536364555359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [03:27<03:27, 207.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=2, dp_p=0.2, wd=0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.5159961549859298\n",
            "Train loss: 1.5200906743174014\n",
            "Train loss: 1.512688124110933\n",
            "Train loss: 1.5067447945153043\n",
            "Train loss: 1.498604822736298\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.530654788017273, Val f1: 0.348596453666687\n",
            "Val loss: 1.5170829845511395, Val f1: 0.34907129406929016\n",
            "Val loss: 1.5101936855757168, Val f1: 0.3511876165866852\n",
            "Val loss: 1.5086731337881707, Val f1: 0.35204648971557617\n",
            "Val loss: 1.5087646530573755, Val f1: 0.3488583266735077\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.630091016109173, Val f1: 0.4030924141407013\n",
            "Val loss: 1.5622203924037792, Val f1: 0.3755277097225189\n",
            "Val loss: 1.5489345818031124, Val f1: 0.35987862944602966\n",
            "Val loss: 1.5322901075536555, Val f1: 0.35528236627578735\n",
            "Val loss: 1.5307380202887715, Val f1: 0.34522905945777893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [06:55<00:00, 207.55s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучше без дропаута"
      ],
      "metadata": {
        "id": "KCVCkaxy9gTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(max(res, key=lambda x: max(x[3]))[-2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7ccawQ6S11",
        "outputId": "786b3c0f-0b47-4f61-d2d4-92ac4b6d2728"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, что будет, если вместо дропаута добавить l2-регуляризацию"
      ],
      "metadata": {
        "id": "LzSysvjk8VRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "params = list(product(*[[[2]], [[150]], [150], [0], [0]]))\n",
        "for p in params:\n",
        "    ps = {'cnn_n': p[0], 'cnn_oc':p[1], 'lstm_dim':p[2], 'dp_l':p[3], 'dp_p':p[4]}\n",
        "    res.append(get_results(2, train_iterator, val_iterator, ps, wd=0.001))\n",
        "print(max(res, key=lambda x: max(x[3]))[-2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59knTx6u6brm",
        "outputId": "e479f7f7-40f2-4ba6-c144-83319af99d89"
      },
      "execution_count": 133,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0.001\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.6701966001276385\n",
            "Train loss: 1.4578872302304144\n",
            "Train loss: 1.3081127577434386\n",
            "Train loss: 1.2429859067970541\n",
            "Train loss: 1.202455156402192\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.409546084571303, Val f1: 0.06628016382455826\n",
            "Val loss: 1.3973092991372813, Val f1: 0.06377993524074554\n",
            "Val loss: 1.3976048155327063, Val f1: 0.06289569288492203\n",
            "Val loss: 1.3946934365606927, Val f1: 0.06287463754415512\n",
            "Val loss: 1.3933884025032546, Val f1: 0.06397398561239243\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.5180768874975352, Val f1: 0.05214604362845421\n",
            "Val loss: 1.4457019920702334, Val f1: 0.06966851651668549\n",
            "Val loss: 1.4289297970329844, Val f1: 0.07257040590047836\n",
            "Val loss: 1.4160482428290628, Val f1: 0.06884511560201645\n",
            "Val loss: 1.4127609021421792, Val f1: 0.06519079208374023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [03:40<03:40, 220.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[2], cnn_oc=[150], lstm_dim=150, dp_l=0, dp_p=0, wd=0.001\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.0914855860827262\n",
            "Train loss: 1.0453127207963362\n",
            "Train loss: 1.0236635015189992\n",
            "Train loss: 1.0074428162533484\n",
            "Train loss: 1.0048687604471886\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.9322593368982014, Val f1: 0.45608171820640564\n",
            "Val loss: 0.9573889079301253, Val f1: 0.43504032492637634\n",
            "Val loss: 0.9531459077934309, Val f1: 0.4324910640716553\n",
            "Val loss: 0.9542001003310794, Val f1: 0.4339636266231537\n",
            "Val loss: 0.954907574043142, Val f1: 0.434422105550766\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.054551784808819, Val f1: 0.4950578510761261\n",
            "Val loss: 1.01061350107193, Val f1: 0.4662399888038635\n",
            "Val loss: 1.004414373781623, Val f1: 0.44722551107406616\n",
            "Val loss: 0.9913714668967507, Val f1: 0.44063645601272583\n",
            "Val loss: 0.9867152934489043, Val f1: 0.4310884475708008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [07:10<00:00, 215.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max(res, key=lambda x: max(x[3]))[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v03UpMP_JCG",
        "outputId": "10159e8b-8010-44e8-c579-0295f92da3c0"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучше без нее.\n",
        "Теперь посмотрим, что будет, если взять фильтры разной длины"
      ],
      "metadata": {
        "id": "3tdf3N6O9wse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "params = list(product(*[[[2, 3, 4]], [[300, 150, 150]], [150], [0], [0]]))\n",
        "for p in params:\n",
        "    ps = {'cnn_n': p[0], 'cnn_oc':p[1], 'lstm_dim':p[2], 'dp_l':p[3], 'dp_p':p[4]}\n",
        "    res.append(get_results(2, train_iterator, val_iterator, ps, wd=0))\n",
        "print(max(res, key=lambda x: max(x[3]))[-2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK9CJTQ6814U",
        "outputId": "b12269bd-1369-4fa8-c70b-a8188298716e"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[2, 3, 4], cnn_oc=[300, 150, 150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 1.7714566536117018\n",
            "Train loss: 1.6326789990715358\n",
            "Train loss: 1.578915651823055\n",
            "Train loss: 1.5571203959452642\n",
            "Train loss: 1.5487697875210984\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.4756701745485004, Val f1: 0.04752263054251671\n",
            "Val loss: 1.4633225980012312, Val f1: 0.04569309949874878\n",
            "Val loss: 1.4652783064483907, Val f1: 0.04531539976596832\n",
            "Val loss: 1.4641246589231285, Val f1: 0.04654628038406372\n",
            "Val loss: 1.4609148605472075, Val f1: 0.04742711782455444\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.5789182552924523, Val f1: 0.0346103198826313\n",
            "Val loss: 1.508352447439123, Val f1: 0.0473824217915535\n",
            "Val loss: 1.4962498996315934, Val f1: 0.05701269581913948\n",
            "Val loss: 1.4777686140754007, Val f1: 0.05819550156593323\n",
            "Val loss: 1.4775391989859983, Val f1: 0.05619310960173607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [04:28<04:28, 268.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: cnn_n=[2, 3, 4], cnn_oc=[300, 150, 150], lstm_dim=150, dp_l=0, dp_p=0, wd=0\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 1.5409216608917504\n",
            "Train loss: 1.5243809088416722\n",
            "Train loss: 1.5100436872140521\n",
            "Train loss: 1.5009611839855903\n",
            "Train loss: 1.494821298493646\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 1.48775739627972, Val f1: 0.3553099036216736\n",
            "Val loss: 1.4737987134767616, Val f1: 0.3521776497364044\n",
            "Val loss: 1.4691219116221963, Val f1: 0.3465982973575592\n",
            "Val loss: 1.4697357085876135, Val f1: 0.3442956209182739\n",
            "Val loss: 1.4676336417148683, Val f1: 0.3494608998298645\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.5735998887282152, Val f1: 0.39856719970703125\n",
            "Val loss: 1.5054659446080525, Val f1: 0.3725178837776184\n",
            "Val loss: 1.4981483366431259, Val f1: 0.3597758114337921\n",
            "Val loss: 1.4826057715849443, Val f1: 0.35674336552619934\n",
            "Val loss: 1.485050298165584, Val f1: 0.3469614088535309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [08:40<00:00, 260.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Итоги\n",
        "\n",
        "Параметры по мере убывания:"
      ],
      "metadata": {
        "id": "dVmYT1Na_1dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(('\\n').join([str((e[-2], e[-1], e[3])) for e in sorted(res, reverse=True, key=lambda x: max(x[3]))]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_khNPQY-cMU",
        "outputId": "3ae30d2d-4c54-4160-f9d3-67b7053ece0a"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}, 0, [tensor(0.4314, device='cuda:0'), tensor(0.4189, device='cuda:0')])\n",
            "({'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}, 0.001, [tensor(0.0642, device='cuda:0'), tensor(0.4273, device='cuda:0')])\n",
            "({'cnn_n': [3], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}, 0, [tensor(0.0007, device='cuda:0'), tensor(0.3597, device='cuda:0')])\n",
            "({'cnn_n': [4], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}, 0, [tensor(0.3597, device='cuda:0'), tensor(0.0647, device='cuda:0')])\n",
            "({'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 1, 'dp_p': 0.2}, 0, [tensor(0.3597, device='cuda:0'), tensor(0.3578, device='cuda:0')])\n",
            "({'cnn_n': [2], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 2, 'dp_p': 0.2}, 0, [tensor(0.3591, device='cuda:0'), tensor(0.3414, device='cuda:0')])\n",
            "({'cnn_n': [2, 3, 4], 'cnn_oc': [300, 150, 150], 'lstm_dim': 150, 'dp_l': 0, 'dp_p': 0}, 0, [tensor(0.0544, device='cuda:0'), tensor(0.3426, device='cuda:0')])\n",
            "({'cnn_n': [3], 'cnn_oc': [150], 'lstm_dim': 150, 'dp_l': 1, 'dp_p': 0.5}, 0, [tensor(0.0021, device='cuda:0'), tensor(0., device='cuda:0')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вероятно, результаты не такие большие, поскольку слишком мало эпох, и из-за этого же не понадобились регуляризация и дропауты (хотя видно, что даже так в некоторых случаях модель переобучилась, возможно, стоит просто еще уменьшить dp и wd). Но с заданным количеством эпох лучше всего брать только 1 фильтр, размером 2 без дропаутов и регуляризации"
      ],
      "metadata": {
        "id": "g39dqLVeAfAH"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW3_nn",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}